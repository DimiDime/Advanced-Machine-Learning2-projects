{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 997,
     "status": "ok",
     "timestamp": 1604482757442,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "lXyR3B1-cst0"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from IPython.core.display import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1477,
     "status": "ok",
     "timestamp": 1604482757927,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "McTkC40Tcst3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1473,
     "status": "ok",
     "timestamp": 1604482757927,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "lopz8Y9Ccst6"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal, norm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8pLlMwKcst8"
   },
   "source": [
    "## Degree in Data Science and Engineering, group 96\n",
    "## Machine Learning 2\n",
    "### Fall 2022\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "# Lab 4. Gaussian Processes\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "**Emilio Parrado Hern√°ndez**\n",
    "\n",
    "Dept. of Signal Processing and Communications\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZqqigYrw4WXW"
   },
   "source": [
    "# Diabetes dataset\n",
    "\n",
    "[Diabetes](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset) is another classic benchmark for regression. Each observation corresponds to a diabetes patient represented by 10 variables and the corresponding target is a score that measures  the disease progression one year after baseline.\n",
    "\n",
    "The variables that form each observation are:\n",
    "- age in years\n",
    "\n",
    "- sex\n",
    "\n",
    "- bmi body mass index\n",
    "\n",
    "- bp average blood pressure\n",
    "\n",
    "- six measures taken from the blood of the patient:\n",
    "  - s1 tc, T-Cells (a type of white blood cells)\n",
    "\n",
    "  - s2 ldl, low-density lipoproteins\n",
    "\n",
    "  - s3 hdl, high-density lipoproteins\n",
    "\n",
    "  - s4 tch, thyroid stimulating hormone\n",
    "\n",
    "  - s5 ltg, lamotrigine\n",
    "\n",
    "  - s6 glu, blood sugar level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2256,
     "status": "ok",
     "timestamp": 1604482758714,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "aIgJdjQGcst_",
    "outputId": "beaf2ca4-ad23-424a-f1f6-8f42ab8fd18c"
   },
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('diabetes.csv', header=0)\n",
    "data.columns = ['AGE', 'SEX', 'BMI', 'BP','TC','LDL','HDL','TCH','LTG','GLU','Y']\n",
    "feature_names = data.columns # list with feature names\n",
    "print(\"Feature names are\")\n",
    "for ii,fn  in enumerate(feature_names[:-1]):\n",
    "    print(\"Column {0:d}: {1}\".format(ii,fn))\n",
    "X = data.values[:,:-1]\n",
    "Y = data['Y'].values\n",
    "print(\"\")\n",
    "print(\"Loaded {0:d} observations with {1:d} columns\".format(X.shape[0], X.shape[1]))\n",
    "print(\"Loaded {0:d} targets\".format(len(Y)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting into training and test set\n",
    "\n",
    "Divide the data set into a training set with $2/3$ of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,Y,random_state=104,test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Gaussian Process Regression initial result\n",
    "\n",
    "Train a Gaussian Process with a composite kernel formed with the sum of:\n",
    "  - RBF kernel:\n",
    "     - `length_scale`= 1.5\n",
    "     - `length_scale_bounds` [1e-2, 1e3]\n",
    "  - White noise kernel:\n",
    "     - `noise_level`=0.1\n",
    "     -`noise_level_bounds` [1e-10, 1e6]\n",
    "     \n",
    "**Print the performance of the model in the test set.**\n",
    "\n",
    "**Print the values of the kernel parameters after the GP optimization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import  RBF, WhiteKernel\n",
    "# GPy: Gaussian processes library\n",
    "import GPy\n",
    "\n",
    "# instantiate the kernels\n",
    "kernel_rbf = RBF(length_scale=1.5, length_scale_bounds(1e-2,1e3))\n",
    "kernel_noise = WhiteNoise(noise_level=0.1, noise_level_bounds(1e-10,1e6))\n",
    "\n",
    "# instantiate the GP (prior on f)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**Discuss about the differences between the kernel parameters before and after optimizing the GP.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Strategies to improve the initial result\n",
    "\n",
    "In this assignment we are going to explore three strategies to improve this initial result\n",
    "\n",
    "1. Scaling the data\n",
    "2. Feature selection\n",
    "3. Kernel design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Scaling the data\n",
    "\n",
    "Repeat the experiment that produced the baseline result scaling the observations with a `MinMaxScaler` and evaluate the impact of this scaling in the performance of the GP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**Did scaling improve the accuracy of the GP?**\n",
    "\n",
    "**Did scaling affect to the final value of the kernel parameters after the optimization?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvtbdKOMohFY"
   },
   "source": [
    "## 2.2 Feature selection\n",
    "\n",
    "The goal in this strategy is to study\n",
    "- if any of the variables is noisy (its presence worsens the performance of the regressors)\n",
    "- if any of the variables is not relevant (its presence or absence does not affect the performance of the regressor, hence you could save resources by skipping its measure\n",
    "- if some of these variables are more critical than the others in the conformation of the score. This way you can gain insights about the main drivers of the disease.\n",
    "\n",
    "We will explore two strategies to perform the feature selection\n",
    "\n",
    "1. Random Forests property `feature_importances_`.  \n",
    "\n",
    "2. GP with an ARD kernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Random Forests `feature_importances_`\n",
    "\n",
    "In Random Forest the variables are individually selected to design the stump test in each branch node of each tree in the forest. Relevant variables will be in general oftenly selected for these test, while noisy or redundant variables will be selected less oftenly.  Besides, since the growing of each tree only considers a subset of the training data, the left-out subset can be used as validation set to evaluate the quality of each stump. In this sense, the most relevant variables will lead to better quality stumps.\n",
    "\n",
    "In the sklearn implementation of Random Forest there is a property `feature_importances_` that is precisely a score in the relevance of the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell write code that\n",
    " 1. Train a Random Forest Regressor with its hyperparameters selected by cross-validation within the following  ranges\n",
    "  - number of trees: 10, 20, 50, 100, 200, 500, 1000\n",
    "  - maximum number of leaves per node: 5, 10, 20, 50\n",
    "  \n",
    " 2. Print the score in the test set  of the Random Forest fitted with the best set of hyperparameters\n",
    " \n",
    " 3. Print the value of `feature_importances_` for each feature in the data set\n",
    " \n",
    " 4. Sort the features in order of decreasing importance in an array called `random_forest_order`\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**Did RF perform in the test set better than GP?**\n",
    "\n",
    "**What are the more relevant features according to RF?**\n",
    "\n",
    "**Are there significant differences in relevance among the features?**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell write code that implements a `for loop` that in each iteration trains a GP with the settings of Section 1 but increasing the number of features in the ordering suggested by `random_forest_order`. \n",
    "\n",
    "Plot the GP accuracy in the test set vs. the number of features used to model the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**What is the best number of features to model the problem?**\n",
    "\n",
    "**Does removing features improve the performance of RF?**\n",
    "\n",
    "**Are there noisy features? (Features that, if present, significantly worsen the performance of the GP)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 GP with an ARD kernel\n",
    "\n",
    "The fitting of a GP endowed with an anisotropic RBF kernel obtains a different value of the `length_scale` for each variable.\n",
    "\n",
    "**Relate the length scale of each variable with its relevance in the predictive function**\n",
    "\n",
    "Hint: Consider how does the output of the predictive function changes as the value of a certain variable $x_k$ changes depending on $l_k^2$.\n",
    "\n",
    "In the next cell write code that fits a GP with an ARD kernel. \n",
    "\n",
    "**Print the lengthscale value of each feature after the kernel has been optimized** Hint, learn to use `kernel_get_params()`.** \n",
    "\n",
    "**Sort the features in order of decreasing importance in an array called `ARD_order`**\n",
    "\n",
    "**Print the score in the test set  of the GP with ARD kernel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell write code that implements a `for loop` that in each iteration trains a GP with an ARD kernel but increasing the number of features in the ordering suggested by `ARD_order`. \n",
    "\n",
    "Plot the GP accuracy in the test set vs. the number of features used to model the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**What is the best number of features to model the problem according to the ARD kernel?**\n",
    "\n",
    "**Does removing features improve the performance of RF?**\n",
    "\n",
    "**Are there noisy features? (Features that, if present, significantly worsen the performance of the GP)**\n",
    "\n",
    "**How does the feature selection suggested by the ARD kernel compare with that suggested by random forest?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pnqv8kpOdLzC"
   },
   "source": [
    "# 4. Exploring sophisticate kernels for the GP\n",
    "\n",
    "The greatest potentiality of GPs are the exploration of different kernels that capture the geometry of the inputs. \n",
    "\n",
    "Besides, the essential kernels can be combined into more sophisticate ones using the addition and multiplication operations.\n",
    "\n",
    "And the most interesting feature, the GP implementation is able to optimize the parameters of the kernel maximizing the likelihood of the observations, what saves the crossvalidation step for optimizing parameters.\n",
    "\n",
    "Read the [section 1.7.5 of this site](https://scikit-learn.org/stable/modules/gaussian_process.html) to learn the different kernels that are implemented in the scikit learn distribution of Gaussian Processes.\n",
    "\n",
    "In this section check at least ten different kernel configurations and evaluate if they improve the kernel evaluated in section 1. Remember this kernel was\n",
    "\n",
    "$$\n",
    "\\kappa_1(\\mathbf x_i, \\mathbf x_j) = \\kappa_r(\\mathbf x_i, \\mathbf x_j) + \\kappa_w(\\mathbf x_i, \\mathbf x_j)\n",
    "$$ where\n",
    "- $\\kappa_r(\\mathbf x_i, \\mathbf x_j)$ is an isotropic RBF kernel with length_scale $l$: \n",
    "$$\n",
    "\\kappa_r(\\mathbf x_i, \\mathbf x_j) = \\exp\\left( -\\frac{\\|\\mathbf x_i - \\mathbf x_j\\|^2}{2l^2}\\right)\n",
    "$$\n",
    "- $\\kappa_w(\\mathbf x_i, \\mathbf x_j)$ is a WhiteKernel that explains the additive noise component\n",
    "$$\n",
    "\\kappa_w(\\mathbf x_i, \\mathbf x_j) = \\left \\{ \\begin{array}{ll} \\sigma_n^2 & \\mbox{if } \\mathbf x_i== \\mathbf x_j \\\\ 0 & \\mbox{otherwise} \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Within the kernel combinations to explore you can include:\n",
    "1. Add a `ConstantKernel` to $\\kappa_1(\\mathbf x_i, \\mathbf x_j)$\n",
    "\n",
    "2. Replace $\\kappa_r(\\mathbf x_i, \\mathbf x_j)$ by an anisotropic RBF in $\\kappa_1(\\mathbf x_i, \\mathbf x_j)$. \n",
    "\n",
    "3. Individual kernels presented in the lecture\n",
    "\n",
    "4. Addition of several kernels\n",
    "\n",
    "5. Multiplication of several kernels\n",
    "\n",
    "6. Use your imagination!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the different kernel combinations to characterize how difficult is the problem at hand in terms of how difficult is to find out a kernel that achieves the best possible result in the test set.\n",
    "\n",
    "For this purpose:\n",
    "1. Group in a same array all the scores in the **test set** achieved by all the kernel combinations that you explore in this section. Consider carrying out this exploration in a programatic fashion.\n",
    "\n",
    "2. Discuss about the range of test accuracies that can be reached with GPs when the kernel is more carefully designed. Depending on the number of different kernels explored you might consider adding to your discussion\n",
    "- minimum, maximum, mean values\n",
    "- standard deviations\n",
    "- percentiles\n",
    "- histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382147,
     "status": "ok",
     "timestamp": 1604483138700,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "-w5KJnordc0Y",
    "outputId": "a7e7438b-864a-4559-d112-91af32f60bb1"
   },
   "outputs": [],
   "source": [
    "#############\n",
    "#           #\n",
    "# YOUR CODE #\n",
    "#           #\n",
    "#############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 389618,
     "status": "ok",
     "timestamp": 1604483146195,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -60
    },
    "id": "7xyVyI6Yy76w"
   },
   "source": [
    "# Items for discussion\n",
    "- Which strategy turned out to be the best in terms of increasing the performance of the GP? \n",
    "- Did this strategy performed significantly better than the others?\n",
    "- Kernel design pushes the GP model further into the **black box method** region, what is the price you pay for sticking to the more interpretable ARD kernel in terms of accuracy? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "lab_regression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1600925966214,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "lXyR3B1-cst0"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from IPython.core.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1600925966486,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "McTkC40Tcst3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 860,
     "status": "ok",
     "timestamp": 1600925966487,
     "user": {
      "displayName": "EMILIO PARRADO HERNANDEZ",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ghu-OvX6osyVq8b7J4Xa7D7HiZ61sPYwKqgPxyJ=s64",
      "userId": "09506376050266996513"
     },
     "user_tz": -120
    },
    "id": "lopz8Y9Ccst6"
   },
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8pLlMwKcst8"
   },
   "source": [
    "## Degree in Data Science and Engineering, group 96\n",
    "## Machine Learning 2\n",
    "### Fall 2022\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "# Lab 2. Support Vector Machines for Classification\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "**Emilio Parrado Hern√°ndez**\n",
    "\n",
    "Dept. of Signal Processing and Communications\n",
    "\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "&nbsp;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='http://www.tsc.uc3m.es/~emipar/BBVA/INTRO/img/logo_uc3m_foot.jpg' width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the dataset\n",
    "\n",
    "In this assignment you will work with an adaptation of the UCI repository dataset [segmentation]('https://archive.ics.uci.edu/ml/datasets/image+segmentation'). It is a binary classification task (labels are $+1$ and $-1$, respectively).\n",
    "\n",
    "The following cell loads the data and constructs the corresponding training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.loadtxt('segmentation.data', delimiter=',')\n",
    "X_train = train_data[:,1:]\n",
    "y_train = train_data[:,0]\n",
    "test_data = np.loadtxt('segmentation.test', delimiter=',')\n",
    "X_test = test_data[:,1:]\n",
    "y_test = test_data[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.- Support Vector Machines with default parameters\n",
    "\n",
    "First approach, train three SVMs, each instantiated with the following configuration:\n",
    "- linear kernel, $C$=1\n",
    "- polynomial kernel, $C$ = 1, `degree` =2\n",
    "- RBF kernel, $C$=1, $\\gamma$ = 1\n",
    "\n",
    "**Complete the code in the two cells below in order to evaluate the performance of these three classifiers in the test set and comment on the accuracy achieved by each method. Discuss your results**\n",
    "\n",
    "Note: In this part do not introduce any preprocessing in the data (such as scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to import needed classes!\n",
    "\n",
    "# Set values for parameters\n",
    "\n",
    "# Instantiate models\n",
    "\n",
    "# Fit models with training data\n",
    "\n",
    "# Evaluate models with test data\n",
    "\n",
    "test_error_default_linear_svc = # accuracy in the test set of the linear svc\n",
    "test_error_default_poly_svc = # accuracy in the test set of the svc with polynomial kernel\n",
    "test_error_default_rbf_svc = # accuracy in the test set of the svc with RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell, do not edit it\n",
    "print(\"Results of default models\")\n",
    "print(\"\")\n",
    "model_name = ['Linear SVC', 'Polynomial SVC', 'RBF SVC']\n",
    "results_lists = [test_error_default_linear_svc, test_error_default_poly_svc, test_error_default_rbf_svc]\n",
    "results = []\n",
    "for name, accuracy in zip(model_name, results_lists):\n",
    "    model.fit(X_train, y_train)\n",
    "    results.append({'method':name,'acc':np.round(accuracy,3)})\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.- Dependence on the parameters\n",
    "## 2.1.- Linear SVC: $C$\n",
    "\n",
    "Study the dependence of the SVC endowed with a linear kernel on the value of $C$. For this goal implement a loop that explores the given range of values for this parameter and plot the test error vs. the value of $C$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_C = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 1e4]\n",
    "lsvc_acc = np.empty(len(v_C)) # to store results\n",
    "\n",
    "# implement for loop that runs for every value in the range of v_C\n",
    "# at each iteration:\n",
    "#   instantiate  the corresponding model\n",
    "#   fit it with training data\n",
    "#   evaluate the model with the test set and store it in lsvc_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not edit this cell, just run it\n",
    "plt.plot(np.array(v_C), lsvc_acc)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('C')\n",
    "plt.grid()\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "**What is the better value for parameter $C$?** \n",
    "\n",
    "**What is the performance of the classifier for that parameter?**\n",
    "\n",
    "**Comment on the sensitivity of the accuracy with respect to $C$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.- SVC with a polynomial kernel: $C$ and $d$\n",
    "\n",
    "In this case you have to study the combination of two parameters. For this purpose scikit learn implements a Grid Search with Cross Validation (`GridSearchCV`). \n",
    "\n",
    "### Grids of hyperparameters\n",
    "\n",
    "This method consists in to form a **grid** with a number of dimensions equal to the number of hyperparameters that one needs to optimize. The size of each dimension of the grid is equal to the number of values in the range of the corresponding hyperparameter. Notice that this method explores **discrete** ranges for each hyperparameter.\n",
    "\n",
    "For the SVC with polynomial kernel we will initially explore the following ranges:\n",
    "\n",
    "- `degree` $ \\in [2, 3, 5]$\n",
    "- `C` $ \\in [0.1, 10, 100]$\n",
    "\n",
    "Notice these ranges determine a $3\\times 3$ grid: 9 different combinations of pairs (`degree`, `C`)  in the grid. \n",
    "\n",
    "In models that depend on a larger number of hyperparameters one has to be careful with the granularity of the ranges as the combinatorial explosion of the size of the grid can be hard to manage.\n",
    "\n",
    "### Cross validation \n",
    "\n",
    "Cross validation is a commonly used procedure in machine learning to simulate the effect of training a model with a set of data and evaluate its generalization capabilities as the performance in a **separate dataset**. \n",
    "\n",
    "The cross validation process involves the following steps:\n",
    "\n",
    "- Randomly partition the training dataset in $N$ disjoint subsets of similar sizes. Each of this subsets is called **fold** in machine learning jargon. Hence the term **N-fold cross validation**.\n",
    "\n",
    "- Let us suppose we have chosen $N=3$ folds. This means the training data has been split in three subsets: $(X_1, Y_1)$, $(X_2, Y_2)$ y $(X_3, Y_3)$. \n",
    "\n",
    "- Create an instance of the model with the corresponding hyperparameters. The cross validation follows with the execution of the following loop\n",
    "\n",
    "    For $n=1,2,\\dots,N$ iterations:  \n",
    "    1. Choose $(X_n,Y_n)$ as **validation set** for iteration $n$\n",
    "    2. Prepare a **training set** for iteration $n$ joining the rest of the subsets (excluding the validation set)\n",
    "    3. Fit the model instance with the training set of step 2\n",
    "    4. Evaluate  the model instance (method `score`) with the validation set of step 1\n",
    "    5. Keep the *score* achieved in the $n$ iteration.\n",
    "\n",
    "- Once the loop is finished, we have $N$ scores, each corresponding to the evaluation of the model fitted in each iteration with the corresponding validation set.\n",
    "- Estimate the **real score** that an instance of the model fitted using all the data would yield in a separate dataset computing the **mean** and **standard deviation** of the $N$ validation scores.\n",
    "\n",
    "Typical values for the number of folds include $N\\in \\{3, 5, 10\\}$\n",
    "\n",
    "\n",
    "###  Cross validation to explore the grid\n",
    "\n",
    "The grid is explored by a loop that visits all its nodes and runs a  **cross validation** to estimate the test performance that the model would yield if it were fitted using the values for hyperparameter that correspond to that node. \n",
    "\n",
    "Once all the nodes of the grid have been cross validated, the procedure outputs the combination of hyperparameters corresponding to the node with the best cross validation performance. \n",
    "\n",
    "###  Grid search in sklearn\n",
    "\n",
    "There is a module in sklearn that implements this algorithm for exploring a grid of hyperparameters with cross validation: [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "\n",
    "### 2.2.1- Initial guess for the values of $C$ and $d$\n",
    "\n",
    "Use the following ranges to get initial values for $C$ and $d$:\n",
    "- `degree` $ \\in [2, 3, 5]$\n",
    "- `C` $ \\in [0.1, 10, 100]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_C_1 = [.01, 10, 100]\n",
    "v_d_1 = [2,3,5]\n",
    "\n",
    "# Check GridSearchCV documentation to construct a GridSearchCV \n",
    "# object with a SVC with polynomial kernel as estimator and a parameter grid \n",
    "# that contains the two ranges given above\n",
    "\n",
    "grid_psvc_1 = # Your code to create the GridSearchCV\n",
    "\n",
    "# fit the GridSearchCV object\n",
    "\n",
    "# evaluate with the test set. Check in the GridSearchCV documentation how to perform this\n",
    "test_error_grid1_psvc = # the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not edit this cell\n",
    "print(\"Best parameters\")\n",
    "print(grid_psvc_1.best_params_)\n",
    "print(\"Best cross-validation score\")\n",
    "print(np.round(grid_psvc_1.best_score_,3))\n",
    "print(\"Test Set Score with best parameters\")\n",
    "print(np.round(test_error_grid1_psvc,3))\n",
    "\n",
    "psvc_initial_best_degree = grid_psvc_1.best_params_['degree']\n",
    "psvc_initial_best_C = grid_psvc_1.best_params_['C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "**Were the default values a good choice? Compare the test set accuracy that you get with the hyperparameters found by `GridSearchCV` with the test set accuracy obtained with the default values.**\n",
    "\n",
    "**Compare the test set accuracy that you get with the hyperparameters found by `GridSearchCV` with the value of the attribute `best_score_` of the already fitted GridSearchCV object, discussing about the generalization capability of the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2- Study of the dependence on the value of $C$ of the accuracy of the SVC endowed with a polynomial kernel.\n",
    "\n",
    "Now run a loop that explores the given range of values for parameter $C$ in a SVC with a polynomial kernel with the best value for $d$ found in 2.2.1. (`psvc_initial_best_degree`) and plot the test set accuracy of the classifier vs. the value of $C$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_C_2 = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 1e4, 1e5, 1e6, 1e7, 1e8]\n",
    "psvc_acc_C = np.empty(len(v_C)) #to store results\n",
    "# your code with the for loop\n",
    "# in each iteration the SVC must have the corresponding value of C and the degree equal to psvc_initial_best_degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(v_C_2), psvc_acc_C)\n",
    "plt.xscale('log')\n",
    "plt.grid()\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**What is the best value of $C$ in this situation? What is the performance of the classifier for that value of $C$?**\n",
    "\n",
    "**Discuss the sensitivity of the accuracy with respect to the value of $C$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3- Study of the dependence on the value of $d$ of the accuracy of the SVC endowed with a polynomial kernel.\n",
    "\n",
    "Now run a loop that explores the given range of values for parameter $d$ in a SVC with a polynomial kernel with the best value for $C$ found in 2.2.1. (`psvc_initial_best_C`) and plot the test set accuracy of the classifier vs. the value of $d$.\n",
    "\n",
    "**Discuss the sensitivity of the accuracy with respect to the value of $d$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_d_2 = [2,3,4,5,6,7,8,9,10]\n",
    "psvc_acc_d = np.empty(len(v_d_2))\n",
    "\n",
    "# your code with the for loop\n",
    "# in each iteration the SVC must have the corresponding value of degree and the C equal to psvc_initial_best_C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(v_d_2), psvc_acc_d)\n",
    "plt.grid()\n",
    "plt.xlabel('d')\n",
    "plt.ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**What is the best value of $d$ in this situation? What is the performance of the classifier for that value of $d$?**\n",
    "\n",
    "**Discuss the sensitivity of the accuracy with respect to the value of $d$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4- GridSearch joint tunning of the values of  $d$ and $C$ for the SVC endowed with a polynomial kernel.\n",
    "\n",
    "The values of $d$ and $C$ interplay in the fitting of the SVC. For this purpose, a greedy sequential tunning can be suboptimal and usually one conducts a grid search of both parameters. \n",
    "\n",
    "### Your comments:\n",
    "\n",
    "**Building on the results of sections 2.2.2 and 2.2.3 design a grid for $d$ and $C$ with a maximum size of 25 values (that is, the product of the lengths of the ranges for $d$ and $C$ must be smaller or equal than 25). Justify the selected grid configuration.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_d_3 = # your range\n",
    "v_C_3 = # your range\n",
    "\n",
    "# construct the parameters for the grid\n",
    "grid_psvc_3 = # your code\n",
    "\n",
    "# instantiate the GridSearchCV object\n",
    "\n",
    "# fit the GridSearchCV object\n",
    "\n",
    "test_error_grid3_psvc = # accuracy in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters\")\n",
    "print(grid_psvc_3.best_params_)\n",
    "print(\"Best cross-validation score\")\n",
    "print(np.round(grid_psvc_3.best_score_,3))\n",
    "print(\"Test Set Score with best parameters\")\n",
    "print(np.round(test_error_grid3_psvc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**Compare the test set accuracy that you obtain with the best hyperparameters found by the grid search with the test set accuracies obtained in sections 2.2.2 and 2.2.3.**\n",
    "\n",
    "**Compare the test set accuracy that you obtain with the best hyperparameters found by the grid search with the value of the attribute `best_score_` of the already fitted GridSearchCV object, discussing about the generalization capability of the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.- SVC with a RBF kernel: $C$ and $\\gamma$\n",
    "\n",
    "### 2.3.1- Initial guess for the values of $C$ and $\\gamma$\n",
    "\n",
    "Use the following ranges to get initial values for $C$ and $\\gamma$:\n",
    "- `gamma` $ \\in [.001, .1, 1]$\n",
    "- `C` $ \\in [0.1, 10, 100]$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_C_4 = [.01, 10, 100]\n",
    "v_g_4 = [1e-2,1e-1,1]\n",
    "\n",
    "# Construct the GridSearchCV object with a SVC with RBF kernel as estimator and \n",
    "# the parameter grid defined above\n",
    "grid_rsvc_4 = # your code\n",
    "\n",
    "# fit the GridSearchCV object\n",
    "\n",
    "test_error_grid4_rsvc = # accuracy of the classifier with the best set of parameters in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters\")\n",
    "print(grid_rsvc_4.best_params_)\n",
    "print(\"Best cross-validation score\")\n",
    "print(np.round(grid_rsvc_4.best_score_,3))\n",
    "print(\"Test Set Score with best parameters\")\n",
    "print(np.round(test_error_grid4_rsvc,3))\n",
    "\n",
    "rsvc_initial_best_gamma = grid_rsvc_4.best_params_['gamma']\n",
    "rsvc_initial_best_C = grid_rsvc_4.best_params_['C']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Your comments:\n",
    "**Were the default values a good choice? Compare the test set accuracy that you get with the hyperparameters found by `GridSearchCV` with the test set accuracy obtained with the default values.**\n",
    "\n",
    "**Compare the test set accuracy that you get with the hyperparameters found by `GridSearchCV` with the value of the attribute `best_score_` of the already fitted GridSearchCV object, discussing about the generalization capability of the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2- Study of the dependence on the value of $C$ of the accuracy of the SVC endowed with a RBF kernel.\n",
    "\n",
    "Now run a loop that explores about 10 values for parameter $C$ in a SVC with a RBF kernel with the best value for $\\gamma$ found in 2.3.1. and plot the test set accuracy of the classifier vs. the value of $C$.\n",
    "\n",
    "**Discuss the sensitivity of the accuracy with respect to the value of $C$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_C_5 = # your range\n",
    "rsvc_acc_C = np.empty(len(v_C))\n",
    "\n",
    "# your code for the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(v_C_5), rsvc_acc_C)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**What is the best value of $C$ in this situation? What is the performance of the classifier for that value of $C$?**\n",
    "\n",
    "**Discuss the sensitivity of the accuracy with respect to the value of $C$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3- Study of the dependence on the value of $\\gamma$ of the accuracy of the SVC endowed with a RBF kernel.\n",
    "\n",
    "Now run a loop that explores the given range of values for parameter $\\gamma$ in a SVC with a RBF kernel with the best value for $C$ found in 2.3.1. and plot the test set accuracy of the classifier vs. the value of $\\gamma$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_g_5 = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "rsvc_acc_g = np.empty(len(v_g_5)) #to store the results\n",
    "\n",
    "# Your code for the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(v_g_5), rsvc_acc_g)\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**What is the best value of $\\gamma$ in this situation? What is the performance of the classifier for that value of $\\gamma$?**\n",
    "\n",
    "**Discuss the sensitivity of the accuracy with respect to the value of $\\gamma$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.4- GridSearch joint tunning of the values of  $\\gamma$ and $C$ for the SVC endowed with a RBF kernel.\n",
    "\n",
    "The values of $\\gamma$ and $C$ interplay in the fitting of the SVC. For this purpose, a greedy sequential tunning can be suboptimal and usually one conducts a grid search of both parameters. \n",
    "\n",
    "**Building on the results of sections 2.3.2 and 2.3.3 design a grid for $\\gamma$ and $C$ with a maximum size of 25 values (that is, the product of the lengths of the ranges for $\\gamma$ and $C$ must be smaller or equal than 25). Justify the selected grid configuration.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_g_6 = # your range\n",
    "v_C_6 = # your range\n",
    "\n",
    "# Create GridSearchCV object\n",
    "grid_rsvc_6 = # your code\n",
    "\n",
    "# fit GridSearchCV object\n",
    "\n",
    "test_error_grid6_rsvc = # evaluate with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters\")\n",
    "print(grid_rsvc_6.best_params_)\n",
    "print(\"Best cross-validation score\")\n",
    "print(np.round(grid_rsvc_6.best_score_,3))\n",
    "print(\"Test Set Score with best parameters\")\n",
    "print(np.round(test_error_grid6_rsvc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:\n",
    "\n",
    "**Compare the test set accuracy that you obtain with the best hyperparameters found by the grid search with the test set accuracies obtained in sections 3.3.2 and 3.3.3.**\n",
    "\n",
    "**Compare the test set accuracy that you obtain with the best hyperparameters found by the grid search with the value of the attribute `best_score_` of the already fitted GridSearchCV object, discussing about the generalization capability of the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.- Scaling the data\n",
    "\n",
    "In this section you will evaluate the impact of scaling the data. Remember some machine learning models benefit from a previous scaling of the input features.\n",
    "\n",
    "## 3.1.- Scaling and linear SVC\n",
    "\n",
    "Repeat the loop of section 2.1 but replacing the SVC by a `Pipeline` object with a `StandardScaler` as first stage and a linear SVC as second stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_lsvc_acc = np.empty(len(v_C)) # to store data inside the loop\n",
    "# YOUR LOOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not edit this cell, just run it\n",
    "plt.plot(np.array(v_C), s_lsvc_acc,label='scaled')\n",
    "plt.plot(np.array(v_C), lsvc_acc, label='not-scaled')\n",
    "plt.xscale('log')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('C')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Your comments:\n",
    "\n",
    "**Comment on the sensitivity of the accuracy with respect to $C$ if the data is previously scaled.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.- Scaling an  SVC with polynomial kernel\n",
    "\n",
    "Repeat the analysis of section 2.2 but scaling the data. Notice that you can combine the `Pipeline` and the `GridSearchCV` in steps 3.2.1 and 3.2.4 to get a cleaner code.\n",
    "\n",
    "### 3.2.1. Inital guess for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline object\n",
    "\n",
    "# Create initial ranges for parameters of the GridSearchCV object\n",
    "\n",
    "# Create a GridSearchCV but with the Pipeline object as estimator\n",
    "grid_sc_psvc = # your code \n",
    "\n",
    "# Fit the GridSearchCV object\n",
    "\n",
    "test_error_grid7_scaled_lsvc = # evaluate the classifier with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters\")\n",
    "print(grid_sc_psvc.best_params_)\n",
    "print(\"Best cross-validation score\")\n",
    "print(np.round(grid_sc_psvc.best_score_,3))\n",
    "print(\"Test Set Score with best parameters\")\n",
    "print(np.round(test_error_grid7_scaled_lsvc,3))\n",
    "\n",
    "sc_psvc_initial_best_degree = grid_sc_psvc.best_params_#complete\n",
    "sc_psvc_initial_best_C = grid_sc_psvc.best_params_#complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.2 Study of the dependence on the value of $C$ of the accuracy of the SVC endowed with a polynomial kernel with the data scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_C_2 = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 1e4, 1e5, 1e6, 1e7, 1e8]\n",
    "grid_sc_psvc_C = np.empty(len(v_C_2)) # to store results\n",
    "\n",
    "# your code with the for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(v_C_2), grid_sc_psvc_C,label='scaled')\n",
    "plt.plot(np.array(v_C_2), psvc_acc_C, label='not-scaled')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid()\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2.3 Study of the dependence on the value of $d$ of the accuracy of the SVC endowed with a polynomial kernel with the data scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_d_2 = [2,3,4,5,6,7,8,9,10]\n",
    "grid_sc_psvc_d = np.empty(len(v_d_2))\n",
    "\n",
    "# your code for the for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(v_d_2), grid_sc_psvc_d,label='scaled')\n",
    "plt.plot(np.array(v_d_2), psvc_acc_d, label='not-scaled')\n",
    "plt.grid()\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4- GridSearch joint tunning of the values of  $d$ and $C$ for the SVC endowed with a polynomial kernel and Scaled Data\n",
    "\n",
    "The values of $d$ and $C$ interplay in the fitting of the SVC. For this purpose, a greedy sequential tunning can be suboptimal and usually one conducts a grid search of both parameters. \n",
    "\n",
    "### Your comments:\n",
    "\n",
    "**Building on the results of sections 3.2.2 and 3.2.3 design a grid for $d$ and $C$ with a maximum size of 25 values (that is, the product of the lengths of the ranges for $d$ and $C$ must be smaller or equal than 25). Justify the selected grid configuration.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_d_7 = # your range\n",
    "v_C_7 = # your range\n",
    "\n",
    "# create parameters for grid\n",
    "\n",
    "# create GridSearchCV object with a pipeline as estimator\n",
    "grid_sc_psvc_2 = # your code\n",
    "\n",
    "test_error_scaled_psvc = # evaluation with test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters\")\n",
    "print(grid_sc_psvc_2.best_params_)\n",
    "print(\"Best cross-validation score\")\n",
    "print(np.round(grid_sc_psvc_2.best_score_,3))\n",
    "print(\"Test Set Score with best parameters\")\n",
    "print(np.round(test_error_scaled_psvc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.- Scaling a  SVC with RBF kernel\n",
    "\n",
    "Repeat the analysis of section 2.3 but scaling the data. Notice that you can combine the `Pipeline` and the `GridSearchCV` in steps 3.3.1 and 3.3.4 to get a cleaner code.\n",
    "\n",
    "### 3.3.1.- Initial guesses for $C$ and $\\gamma$\n",
    "Repeat the study of section 2.3.1 but replacing the classifiers with a pipeline that combines a scaling of the data with the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline object\n",
    "\n",
    "# Create initial ranges for parameters of the GridSearchCV object\n",
    "\n",
    "# Create a GridSearchCV but with the Pipeline object as estimator\n",
    "grid_sc_rsvc = # your code \n",
    "\n",
    "# Fit the GridSearchCV object\n",
    "\n",
    "test_error_grid9_scaled_rsvc = # evaluate the classifier with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters\")\n",
    "print(grid_sc_rsvc.best_params_)\n",
    "print(\"Best cross-validation score\")\n",
    "print(np.round(grid_sc_rsvc.best_score_,3))\n",
    "print(\"Test Set Score with best parameters\")\n",
    "print(np.round(test_error_grid9_scaled_rsvc,3))\n",
    "\n",
    "sc_rsvc_initial_best_gamma = grid_sc_rsvc.best_params_#complete\n",
    "sc_rsvc_initial_best_C = grid_sc_rsvc.best_params_#complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.2 Study of the dependence on the value of $C$ of the accuracy of the SVC endowed with a RBF kernel with the data scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_C_2 = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000, 1e4, 1e5, 1e6, 1e7, 1e8]\n",
    "pipe_rsvc_acc_C = np.empty(len(v_C_2)) # to store results\n",
    "\n",
    "# for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(v_C_2), pipe_rsvc_acc_C, label='scaled')\n",
    "plt.plot(np.array(v_C_2), rsvc_acc_C, label='not-scaled')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('C')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3.3 Study of the dependence on the value of $\\gamma$ of the accuracy of the SVC endowed with a RBF kernel with the data scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_g_5 = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10]\n",
    "pipe_sc_rsvc_g = np.empty(len(v_g_5))\n",
    "\n",
    "# your code for the for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(v_g_5), pipe_sc_rsvc_g, label='scaled')\n",
    "plt.plot(np.array(v_g_5), rsvc_acc_g, label='not-scaled')\n",
    "plt.grid()\n",
    "plt.xscale('log')\n",
    "plt.xlabel('gamma')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4- GridSearch joint tunning of the values of  $\\gamma$ and $C$ for the SVC endowed with a RBF kernel and Scaled Data\n",
    "The values of $\\gamma$ and $C$ interplay in the fitting of the SVC. For this purpose, a greedy sequential tunning can be suboptimal and usually one conducts a grid search of both parameters. \n",
    "\n",
    "**Building on the results of sections 3.3.2 and 3.3.3 design a grid for $\\gamma$ and $C$ with a maximum size of 25 values (that is, the product of the lengths of the ranges for $\\gamma$ and $C$ must be smaller or equal than 25). Justify the selected grid configuration.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_g_8 = # your range\n",
    "v_C_8 = # your range\n",
    "\n",
    "\n",
    "# create parameters for grid\n",
    "\n",
    "# create GridSearchCV object with a pipeline as estimator\n",
    "grid_sc_rsvc_2 = # your code\n",
    "\n",
    "test_error_grid_scaled_rsvc = # evaluate with the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters\")\n",
    "print(grid_sc_rsvc_2.best_params_)\n",
    "print(\"Best cross-validation score\")\n",
    "print(np.round(grid_sc_rsvc_2.best_score_,3))\n",
    "print(\"Test Set Score with best parameters\")\n",
    "print(np.round(test_error_grid_scaled_rsvc,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Items for discussion\n",
    "- Is there any kernel that performs significantly better than the others?\n",
    "- Is it reasonably easy to find wide ranges where the hyperparameters offer decent performances?\n",
    "- Does scaling significantly change the performance of the models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "lab_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
